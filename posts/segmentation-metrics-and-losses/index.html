<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Segmentation: Metrics and Losses in 40 Easy Steps | Computer Visions</title><meta name=keywords content="segmentation,metrics,losses,deep learning,computer vision"><meta name=description content="Semantic segmentation is a crucial component of visual perception, with high demand in both low-latency and precision-focused applications. I have decided to create an overview of the current state of this task.
The first part contains a brief overview of various approaches to evaluating segmentation model performance. Choosing the right set of metrics is often the first step when developing a segmentation system. Understanding the common performance measurements helps clarify the task at hand and provides insights into the challenges that arise when creating such a system."><meta name=author content="Andrey Buzin"><link rel=canonical href=https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/><meta name=google-site-verification content="G-HHKP0NFYPS"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://deepbuzin.github.io/icon128.png><link rel=icon type=image/png sizes=16x16 href=https://deepbuzin.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://deepbuzin.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://deepbuzin.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://deepbuzin.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><style>body{font-size:12pt}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-HHKP0NFYPS"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HHKP0NFYPS",{anonymize_ip:!1})}</script><meta property="og:title" content="Segmentation: Metrics and Losses in 40 Easy Steps"><meta property="og:description" content="Semantic segmentation is a crucial component of visual perception, with high demand in both low-latency and precision-focused applications. I have decided to create an overview of the current state of this task.
The first part contains a brief overview of various approaches to evaluating segmentation model performance. Choosing the right set of metrics is often the first step when developing a segmentation system. Understanding the common performance measurements helps clarify the task at hand and provides insights into the challenges that arise when creating such a system."><meta property="og:type" content="article"><meta property="og:url" content="https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-10T12:00:00+03:00"><meta property="article:modified_time" content="2023-04-10T12:00:00+03:00"><meta property="og:site_name" content="This is Computer Visions"><meta name=twitter:card content="summary"><meta name=twitter:title content="Segmentation: Metrics and Losses in 40 Easy Steps"><meta name=twitter:description content="Semantic segmentation is a crucial component of visual perception, with high demand in both low-latency and precision-focused applications. I have decided to create an overview of the current state of this task.
The first part contains a brief overview of various approaches to evaluating segmentation model performance. Choosing the right set of metrics is often the first step when developing a segmentation system. Understanding the common performance measurements helps clarify the task at hand and provides insights into the challenges that arise when creating such a system."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://deepbuzin.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Segmentation: Metrics and Losses in 40 Easy Steps","item":"https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Segmentation: Metrics and Losses in 40 Easy Steps","name":"Segmentation: Metrics and Losses in 40 Easy Steps","description":"Semantic segmentation is a crucial component of visual perception, with high demand in both low-latency and precision-focused applications. I have decided to create an overview of the current state of this task.\nThe first part contains a brief overview of various approaches to evaluating segmentation model performance. Choosing the right set of metrics is often the first step when developing a segmentation system. Understanding the common performance measurements helps clarify the task at hand and provides insights into the challenges that arise when creating such a system.","keywords":["segmentation","metrics","losses","deep learning","computer vision"],"articleBody":"Semantic segmentation is a crucial component of visual perception, with high demand in both low-latency and precision-focused applications. I have decided to create an overview of the current state of this task.\nThe first part contains a brief overview of various approaches to evaluating segmentation model performance. Choosing the right set of metrics is often the first step when developing a segmentation system. Understanding the common performance measurements helps clarify the task at hand and provides insights into the challenges that arise when creating such a system.\nThe second part delves into loss functions, a topic closely connected to metrics. Selecting an appropriate loss function greatly impacts a model’s ability to improve on a specific metric.\nAn overview of the architectures is not included in this post but is likely to be covered in Part 2.\nNotation Syntax Description $\\Omega = \\{x_1,…,x_N\\}$ Image represented by a point set $N=|\\Omega|$ Number of pixels $C$ Number of classes $S_g=\\left\\{y_i| y_i\\in\\{0,1\\} \\text{ or }[0,1]\\right\\}_i^N$ Partition of the image representing ground truth segmentation $S_t=\\left\\{\\hat{y}_i| \\hat{y}_i\\in\\{0,1\\} \\text{ or }[0,1]\\right\\}_i^N$ Partition of the image representing predicted segmentation $S^1=\\{y_i| y_i=1\\}$ Foreground of the segmentation $S^0=\\{y_i| y_i=0\\}$ Background of the segmentation $\\mathbf{1}_{\\text{condition}}$ Binary indication function $A \\odot B$ Hadamard (element-wise) product Metrics When evaluating a predicted segmentation against the ground truth, various factors need to be taken into account, depending on the objective. These factors include overall alignment and shape errors, volume errors, boundary errors, density, and the general quality of the segmentation. It is important to note that some metrics are sensitive to outliers, class imbalance, segmentation density, and other specific details.\nThis section is based on the survey by [Taha et al. 2015] and offers definitions for 20 metrics categorized into six groups: overlap-based, volume-based, pair counting-based, information theory-based, probabilistic, and spatial distance-based. While insights on the behavior of each specific metric are provided, readers interested in a more comprehensive overview are encouraged to refer to the original study.\nCommon definitions Let the image be represented by a point set $\\Omega = \\{x_1,…,x_N\\}$ with $|\\Omega| = w \\times h \\times d = N$, where $w$, $h$ and $d$ are the width, height and depth of the grid on which the image is defined.\nLet the ground truth segmentation be represented by the partition $S_g=\\left\\{y_i|y_i\\in\\{0,1\\} \\text{ or }(0,1)\\right\\}_i^N$ of $\\Omega$. Let the predicted segmentation be represented by the partition $S_t=\\left\\{\\hat{y}_i|\\hat{y}_i\\in\\{0,1\\} \\text{ or }(0,1)\\right\\}_i^N$. Assume that $S^1=\\{y_i|y_i=1\\}$ is the foreground and $S^0=\\{y_i|y_i=0\\}$ is the background.\nBasic components: confusion matrix For two crisp segmentations $S_g$ and $S_t$, the confusion matrix consists of the four basic components that reflect the overlap between them, namely True Positive, False Positive, False Negative and True Negative. These components are counting based, which is reflected in the case when $\\hat{y_i}\\in\\{0,1\\}$. However, I abuse the notation here by pointing out that their soft probabilistic approximations are defined by the same formulas when $\\hat{y}_i\\in[0,1]$. This distinction will be relevant when discussing loss functions, as the latter are differentiable, whereas the former are not.\n$$ \\begin{align} TP\u0026=\\sum_i^N\\hat{y}_i y_i\\\\ FP\u0026=\\sum_i^N\\hat{y}_i(1-y_i)\\\\ FN\u0026=\\sum_i^N(1-\\hat{y}_i) y_i\\\\ TN\u0026=\\sum_i^N(1-\\hat{y}_i)(1-y_i) \\\\ \\end{align} $$\nNote that the fuzzy definitions are available in the original study [Taha et al. 2015]. I have omitted them for clarity, but it should be assumed that all metrics based on these components are applicable in the fuzzy case as well.\nWhen evaluating metrics defined using the confusion matrix, it is important to consider that some segmentation algorithms may produce segmentations with lower density. These resulting segmentations may have numerous uniformly distributed holes in the foreground, which are counted as false negatives. Such segmentations will be scored lower than denser counterparts with equivalent volume and alignment. This effect may be undesirable if the objective is to prioritize boundary precision over density. In these cases, spatial distance-based metrics are a more suitable choice.\nAnother aspect to consider, specifically for metrics that include TN, is that they reward segmentations with bias towards background and penalize larger segments more heavily. This can be particularly disadvantageous when aiming for maximum recall.\nOverlap-based metrics A shared characteristic of all overlap-based metrics is that they do not account for the pixel positions of false positives and false negatives. This is because these pixels are not part of the overlapping region, causing overlap-based metrics to fail in reflecting the distance-wise magnitude of the error. As a result, these metrics are not suitable for situations where low or zero overlap is likely to occur due to alignment errors, such as with class imbalance or when the segmented regions are small (in at least one dimension). As mentioned earlier, they also penalize low-density segmentations.\nHowever, overlap-based metrics are suitable when dealing with outliers and generally low-quality segmentations, as they reflect poor alignment and low overlap. They are also appropriate when prioritizing volume.\nDice score and Intersection over Union The Dice coefficient is the most commonly used metrics in segmentation.\n$$ \\begin{equation} Dice = \\frac{2\\big|S_g^1 \\cap S_p^1\\big|}{\\big| S_g^1 \\big| + \\big| S_p^1 \\big|} = \\frac{2TP}{2TP + FP + FN} \\end{equation} $$\nSimilarly, Intersection over Union (IoU), also known as the Jaccard index, is defined as\n$$ \\begin{equation} IoU= \\frac{\\big|S_g^1 \\cap S_p^1\\big|}{\\big| S_g^1 \\cup S_p^1 \\big|}=\\frac{TP}{TP+FP+FN} \\end{equation} $$\nThe two metrics can be shown to be related:\n$$ \\begin{align} IoU \u0026= \\frac{Dice}{2 - Dice}, \u0026 Dice \u0026= \\frac{2IoU}{1+IoU} \\end{align} $$\nTherefore there’s no additional information gain in considering both of them at the same time.\nTPR, TNR, FPR, FNR True Positive Rate (TPR, Recall, Sensitivity) measures the proportion of positive pixels in the ground truth that are identified as positive in the evaluated segmentation. True Negative Rate (TNR, Specificity) measures the proportion of negative (background) pixels in the ground truth that are identified as negative in the evaluated segmentation. Both of these metrics are highly sensitive to segment size, as they penalize errors in small segments more severely than in larger ones.\n$$ \\begin{equation} TPR = Recall = Sensitivity = \\frac{TP}{TP+FN} \\end{equation} $$\n$$ \\begin{equation} TNR = Specificity = \\frac{TN}{TN+FP} \\end{equation} $$\nIn some applications, missing regions are more undesirable than added regions, requiring the segmentation to include at least all true positives and thus prioritizing recall over precision.\nFalse Positive Rate (FPR) and False Negative Rate are directly related to TPR and TNR, respectively. This implies that one should select at most one metric from each pair for evaluation.\n$$ \\begin{equation} FPR = \\frac{FP}{FP+TN} = 1 - TNR \\end{equation} $$\n$$ \\begin{equation} FNR = \\frac{FN}{FN+TP} = 1 - TPR \\end{equation} $$\nPrecision Precision, also know as the Positive Predictive Value (PPV) is used to calculate F-Measure.\n$$ \\begin{equation} Precision = PPV = \\frac{TP}{TP+FP} \\end{equation} $$\n$F_{\\beta}$-Measure $F_{\\beta}$-Measure represents the trade-off between precision and recall:\n$$ \\begin{equation} F_{\\beta} = \\frac{(\\beta^2 + 1)\\cdot Precision \\cdot Recall}{\\beta^2 \\cdot Precision \\cdot Recall} \\end{equation} $$\nAt $\\beta=1$ it becomes an $F_1$-Measure, also called the harmonic mean. By substitution it can be shown to be equivalent to the Dice index.\n$$ \\begin{equation} F_1 = \\frac{2\\cdot Precision \\cdot Recall}{Precision \\cdot Recall} = Dice \\end{equation} $$\nGlobal Consistency Error Let $R(S,x)$ be a set of all the pixels that belong to the same segment as $x$ in the segmentation $S$ (foreground or background). The error between two segmentations $S_1$ and $S_2$ at the pixel $x$ is defined as follows:\n$$ \\begin{equation} LRE(S_1, S_2, x) = \\frac{\\left| R(S_1, x) \\setminus R(S_2, x) \\right|}{\\left| R(S_1, x) \\right|} \\end{equation} $$\nNote that it’s not symmetric. Now, the Global Consistency Error (GCE) can be defined as the average $LRE$ over all pixels:\n$$ \\begin{equation} GCE(S_p, S_g) = \\frac1N\\min \\left\\{ \\sum_i^N LRE(S_p, S_g, x_i), \\sum_i^N LRE(S_g, S_p, x_i) \\right\\} \\end{equation} $$\nOr, in terms of the confusion matrix,\n$$ \\begin{equation} \\begin{split} GCE(S_p, S_g) = \\frac1N\\min \\bigg\\{ \u0026\\frac{FN(FN+2TP)}{TP+FN} + \\frac{FP(FP+2TN)}{TN+FP}, \\\\[15pt] \u0026\\frac{FP(FP+2TP)}{TP+FP} + \\frac{FN(FN+2TN)}{TN+FN} \\bigg\\} \\end{split} \\end{equation} $$\nNote on the multiple label case In many real-world scenarios, it’s common to compare segmentations with multiple labels. In practice, a typical approach is to compare each label individually and then calculate the average across all labels. Assume $M$ is a metric function defined for the binary case,\n$$ \\begin{equation} M_{ml}=\\frac1C\\sum_cM^c \\end{equation} $$\nwhere $C$ is the number of classes.\nGeneralized Dice score The approach presented above, however, ignores class imbalance. In literature one can find a weighted multi-label variant of the Dice score called the Generalized Dice score:\n$$ \\begin{equation} GD=\\frac{2\\sum_cw_c\\big|S_g^c \\cap S_p^c\\big|}{\\sum_cw_c\\Big[\\big| S_g^c \\big| + \\big| S_t^c \\big|\\Big]} = \\frac{2\\sum_cw_cTP_c}{\\sum_cw_c[2TP_c + FP_c + FN_c]} \\end{equation} $$\nwhere $w_c=\\left|S_g^c\\right|^{-2} = \\left(TP_c+FN_c\\right)^{-2}$. Fuzzy definition can be found in the original study. One can obtain a generalized IoU score by applying (7).\nVolume based metrics Volumetric Similarity Volumetric Similarity (VS) is defined as $1-VD$, where $VD$ is the volumetric distance. We calculate it as the difference between the absolute volumes of the segments divided by the sum of these volumes.\n$$ \\begin{equation} VS = 1- \\frac{\\Big| \\left| S_p^1 \\right| - \\left| S_g^1 \\right| \\Big|}{\\left| S_p^1 \\right|+ \\left| S_g^1 \\right|} = 1 - \\frac{\\left|FN - FP\\right|}{2TP+FP+FN} \\end{equation} $$\nNote that even though we expressed it through the confusion matrix, VS is not considered to be an overlap metric since it only compares the absolute volumes. In fact, it can reach 1 even at zero overlap.\nIt is important to keep in mind that VS only compares the volume of the segments and carries no information about their shape or alignment.\nPixel pair groups In order to advance to the next group of metrics, we are going to define the base pair-counting elements. Let $P$ be the set of $\\frac{n(n-1)}{2}$ pairs representing all pixel pairs in $X$. We’re going to classify each pair $(x_i,x_j)\\in P\\text{, }i,j\\in[0,N]$ into one of four categories based on which subset (foreground or background) those pixels are placed to according to each of the segmentations. To avoid the $O(n^2)$ runtime, it is shown to be possible to calculate these categories using the values in the confusion matrix.\nFor pairs $(x_i,x_j)$ where $x_i$ and $x_j$ are placed in the same subset in both $S_g$ and $S_t$:\n$$ \\begin{equation} a = \\frac{1}{2}\\bigg[TP(TP-1)+FP(FP-1)+TN(TN-1)+FN(FN-1)\\bigg] \\end{equation} $$\nWhere $x_i$ and $x_j$ are placed in the same subset in $S_g$, but in different subsets in $S_t$:\n$$ \\begin{equation} b = \\frac{1}{2}\\bigg[ (TP+FN)^2+(TN+FP)^2-\\Big(TP^2+TN^2+FP^2+FN^2\\Big) \\bigg] \\end{equation} $$\nWhere $x_i$ and $x_j$ are placed in different subsets in $S_g$, but in the same subset in $S_t$:\n$$ \\begin{equation} c = \\frac{1}{2}\\bigg[ (TP+FP)^2+(TN+FN)^2-\\Big(TP^2+TN^2+FP^2+FN^2\\Big) \\bigg] \\end{equation} $$\nFinally, where $x_i$ and $x_j$ are placed in different subsets in both $S_g$ and $S_t$:\n$$ \\begin{equation} d = \\frac{N(N-1)}{2}-(a+b+c) \\end{equation} $$\nPair counting based metrics Rand Index The Rand Index (RI) was originally proposed for measuring the similarity between clusterings and was later adapted for classification.\n$$ \\begin{equation} RI(S_g, S_p) = \\frac{a+b}{a+b+c+d} \\end{equation} $$\nAdjusted Rand Index The Adjusted Rand Index (ARI) is a modification of RI with a correction for chance. It can be expressed by the pair-counting groups as:\n$$ \\begin{equation} ARI(S_g, S_p) = \\frac{2(ad-bc)}{c^2+b^2+2ad+(a+d)(c+b)} \\end{equation} $$\nHaving the built-in chance adjustment makes ARI a good choice for when there is a heavy class imbalance.\nInformation theory based metrics Assuming $N=TP+FP+FN+TN$ is the total number of pixels, the probability of a randomly sampled pixel belonging to each of the classes in either segmentation can be expressed using the confusion matrix as follows:\n$$ \\begin{align} p\\left(S_g^1\\right) \u0026= \\frac{TP+FN}{N} \\\\[15pt] p\\left(S_g^0\\right) \u0026= \\frac{TN+FP}{N} \\\\[15pt] p\\left(S_t^1\\right) \u0026= \\frac{TP+FP}{N} \\\\[15pt] p\\left(S_t^0\\right) \u0026= \\frac{TN+FN}{N} \\end{align} $$\nThe joint probabilities for some are given by\n$$ \\begin{align} p\\left(S_g^1,S_p^1\\right) = \\frac{TP}{N} \\\\[15pt] p\\left(S_g^1,S_p^0\\right) = \\frac{FN}{N} \\\\[15pt] p\\left(S_g^0,S_p^1\\right) = \\frac{FP}{N} \\\\[15pt] p\\left(S_g^0,S_p^0\\right) = \\frac{TN}{N} \\end{align} $$\nHaving the definitions above we can express the marginal entropy between the regions\n$$ \\begin{equation} H(S)=-\\sum_i p\\left(S^i\\right)\\log p\\left(S^i\\right) \\end{equation} $$\nand the joint entropy between the segmentations\n$$ \\begin{equation} H(S_1,S_2)=-\\sum_{i,j} p\\left(S_1^i, S_2^j\\right)\\log p\\left(S_2^i,S_2^j\\right) \\end{equation} $$\nMutual Information The Mutual Information (MI) measures the reduction in uncertainty of one variable when the other one is known.\n$$ \\begin{equation} MI(S_g,S_p)=H(S_g)+H(S_p)-H(S_g,S_p) \\end{equation} $$\nNote that MI essentially measures how much information the segmentations have in common and therefore rewards high recall.\nVariation of Information The Variation of Information (VoI) measures the amount of information lost when transitioning from one variable to the other.\n$$ \\begin{equation} VoI(S_g,S_p)=H(S_g)+H(S_p)-2MI(S_g,S_p) \\end{equation} $$\nProbabilistic metrics Intraclass Correlation Coefficient\nIntraclass Correlation Coefficient (ICC) is sometimes used as a measure of consistency between two segmentations, specifically in the medical imaging domain.\n$$ \\begin{equation} ICC = \\frac{\\sigma_S^2}{\\sigma_S^2+\\sigma_{\\epsilon}^2} \\end{equation} $$\nHere $\\sigma_S$ denotes variance caused by differences between segmentations and $\\sigma_{\\epsilon}$ denotes variance cause by differences between the points within each segmentation. For segmentations $S_g$ and $S_t$, we express it via the mean squares between segmentations $MS_b$ and the mean squares within the segmentations $MS_w$:\n$$ \\begin{gather} \u0026ICC = \\frac{MS_b-MS_w}{MS_b+MS_w} \\end{gather} $$\nwhere\n$$ \\begin{align} \u0026MS_b = \\frac{2}{N-1}\\sum_i \\left(\\frac{\\hat{y}_i+y_i}{2}-\\mu\\right)^2 \\\\[15pt] \u0026MS_w = \\frac1N\\sum_i\\left(y_i-\\frac{\\hat{y}_i+y_i}{2}\\right)^2 + \\left(\\hat{y}_i-\\frac{\\hat{y}_i+y_i}{2}\\right)^2 \\\\ \\end{align} $$\nHere $\\mu$ is the mean of means of the two segmentations.\nProbabilistic Distance The Probabilistic Distance (PBD) [Guido et al. 2001] is designed as a measure of distance between two fuzzy segmentations.\n$$ \\begin{equation} PBD(S_g,S_p)=\\frac{\\sum_i \\big|y_i-\\hat{y}_i\\big|}{2\\sum_i y_i \\hat{y}_i} \\end{equation} $$\nNote that in contrast to Dice, the PBD over-penalizes false positives and false negatives, as they both reduce the denominator and increase the numerator to the point where PBD reaches infinity at zero overlap. This results in PBD strongly reflecting alignment errors, i.e., when the volume is correct and the overlap is low.\nCohen’s Kappa Coefficient The Cohen’s Kappa Coefficient is a robust measure of agreement between the samples that takes into account the agreement caused by chance.\n$$ \\begin{equation} \\kappa=\\frac{2(TP\\cdot TN-FN\\cdot FP)}{(TP+FP)(FP+TN)+(TP+FN)(FN+TN)} \\end{equation} $$\nChance adjustment makes Kappa a good choice when there is high class imbalance.\nROC AUC The ROC curve is a plot of TPR against FPR at every possible threshold. The area under the ROC curve reflects the probability for the classifier to rank a positive example higher than the negative one. It is possible to calculate a rough estimate of the AUC for a single measurement case.\n$$ \\begin{equation} AUC=1-\\frac{FPR+FNR}{2} \\end{equation} $$\nNote that calculating AUC this way is generally not recommended, since it significantly underestimates the value.\nSpatial distance based metrics Spatial distance-based metrics possess the notable property of taking into account pixel positions outside the overlap region, as well as those within it. This enables them to offer more meaningful rankings in situations where overlap is likely to be low, such as with small object segmentations and low-density segmentations. These metrics are also applicable when prioritizing boundary or overall alignment and in cases involving low-quality segmentations.\nHausdorff Distance The Hausdorff Distance (HD) is defined as the maximum distance from a point in one set to the nearest point in the other set.\n$$ \\begin{equation} HD(S_g^i,S_p^i)=\\max\\big(h(S_g^i, S_p^i),h(S_p^i, S_g^i)\\big) \\end{equation} $$\nwhere $h(A,B)$ is the directed Hausdorff distance given by\n$$ \\begin{equation} h(A,B)=\\max_{a\\in A}\\min_{b\\in B} |a-b| \\end{equation} $$\nHD can be viewed as an indicator of the largest segmentation error. It is computed between boundaries of the ground truth and the predicted segmentation.\nAlgorithms have been developed that calculate the HD in near-linear time. Note that HD is sensitive to outliers, which is why it is recommended to use the quantile version instead of applying it directly.\nAverage Hausdorff Distance The Average Hausdorff Distance is the HD averaged over all points. It is known to be more robust than the original.\n$$ \\begin{equation} AHD(S_g^i,S_p^i)=\\max\\big(d(S_g^i,S_p^i),d(S_p^i,S_g^i)\\big) \\end{equation} $$\nwhere $d(A,B)$ is the directed average Hausdorff distance that is defined as\n$$ \\begin{equation} d(A,B)=\\frac{1}{N}\\sum_{a\\in A}\\min_{b\\in B}|a-b| \\end{equation} $$\nMahalanobis Distance The Mahalanobis Distance (MD) is a metric that measures the distance between a point and a distribution while considering the shape of the distribution. However, when comparing image segmentations, we need to measure the distance between two distributions. To achieve this, we calculate the distance between their means as follows:\n$$ \\begin{equation} MD(S_g^i,S_p^i) = \\sqrt{(\\mu_{g,i}-\\mu_{t,i})^T K^{-1}(\\mu_{g,i}-\\mu_{t,i})} \\end{equation} $$\nwhere $\\mu_{g,i}$ and $\\mu_{t,i}$ are the means for the $i$th category in the respective segmentation, and their common covariance matrix is given by\n$$ \\begin{equation} K=\\frac{n_{g,i} K_{g,i} + n_{t,i} K_{t,i}}{n_{g,i} + n_{t,i}} \\end{equation} $$\nHere $K_{g,i}$ and $K_{t,i}$ are respective covariance matrices and $n_{g,i},n_{t,i}$ are the numbers of pixels.\nNote that MD ignores boundary details and considers only general shape and alignment, i. e. the two ellipsoids that best represent the segmentations.\nLoss functions After examining various popular metrics and gaining insight into the challenges involved, we can proceed to select the appropriate loss function. The choice of loss function can profoundly influence a model’s capacity to learn the nuances of a task, which often includes class imbalance, disparate object sizes, and intricate boundary delineation.\nThis section draws in part on Jun Ma’s survey [Jun Ma 2020], with some additions. It does not maintain a one-to-one correspondence with the metric section, as numerous metrics discussed earlier are non-differentiable and thus unsuitable for backpropagation. However, some have been approximated with differentiable functions, making them compatible with gradient-based optimization.\nDistribution-based loss Distribution-based loss functions treat both the ground truth and predicted segmentation as probability distributions, with the goal of minimizing their differences. The cross-entropy serves as the foundational loss function in this category, from which others are derived.\nCross entropy loss Cross-entropy (CE) is related to the Kullback-Leibler (KL) divergence, a metric that quantifies the dissimilarity between two probability distributions. In machine learning, when the data distribution is determined by the training set, minimizing KL divergence is equivalent to minimizing CE.\n$$ \\begin{equation} L_{CE}=-\\frac1N\\sum_i\\sum_c y_i^c \\log \\hat{y}_i^c \\end{equation} $$\nwhere $g_i^c$ indicates if class label $c$ is a correct classification for the pixel $i$, and $s_i^c$ is the corresponding predicted probability.\nWeighted cross entropy (WCE) [Ronneberger et al. 2015] is a popular modification to the CE:\n$$ \\begin{equation} L_{WCE}=-\\frac1N\\sum_i\\sum_c w_c y_i^c \\log \\hat{y}_i^c \\end{equation} $$\nwhere $w_c$ is the corresponding class weight. It is common to set $w_c$ as inversely proportional to the class frequency to balance out majority classes.\nTopK loss TopK loss [Wu et al. 2016] focuses the training on the hard pixels by discarding the ones that model evaluates with enough confidence. Notably this approach automatically balances the biased data by skipping the over-learned majority class.\n$$ \\begin{equation} L_{TopK}= -\\frac{\\sum_i\\sum_c \\bm{1}_{\\hat{y}_i^c","wordCount":"5095","inLanguage":"en","datePublished":"2023-04-10T12:00:00+03:00","dateModified":"2023-04-10T12:00:00+03:00","author":{"@type":"Person","name":"Andrey Buzin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/"},"publisher":{"@type":"Organization","name":"Computer Visions","logo":{"@type":"ImageObject","url":"https://deepbuzin.github.io/icon128.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://deepbuzin.github.io/ accesskey=h title="Computer Visions (Alt + H)"><img src=https://deepbuzin.github.io/icon128.png alt aria-label=logo height=35>Computer Visions</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://deepbuzin.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://deepbuzin.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://deepbuzin.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://deepbuzin.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://deepbuzin.github.io/posts/>Posts</a></div><h1 class=post-title>Segmentation: Metrics and Losses in 40 Easy Steps</h1><div class=post-meta><span title='2023-04-10 12:00:00 +0300 +0300'>April 10, 2023</span>&nbsp;·&nbsp;24 min&nbsp;·&nbsp;5095 words&nbsp;·&nbsp;Andrey Buzin</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#notation>Notation</a></li><li><a href=#metrics>Metrics</a><ul><li><a href=#common-definitions>Common definitions</a></li><li><a href=#basic-components-confusion-matrix>Basic components: confusion matrix</a></li><li><a href=#overlap-based-metrics>Overlap-based metrics</a></li><li><a href=#note-on-the-multiple-label-case>Note on the multiple label case</a></li><li><a href=#volume-based-metrics>Volume based metrics</a></li><li><a href=#pixel-pair-groups>Pixel pair groups</a></li><li><a href=#pair-counting-based-metrics>Pair counting based metrics</a></li><li><a href=#information-theory-based-metrics>Information theory based metrics</a></li><li><a href=#probabilistic-metrics>Probabilistic metrics</a></li><li><a href=#spatial-distance-based-metrics>Spatial distance based metrics</a></li></ul></li><li><a href=#loss-functions>Loss functions</a><ul><li><a href=#distribution-based-loss>Distribution-based loss</a></li><li><a href=#overlap-based-loss>Overlap-based loss</a></li><li><a href=#compound-loss>Compound loss</a></li><li><a href=#boundary-based-loss>Boundary-based loss</a></li><li><a href=#topological-loss>Topological loss</a></li></ul></li><li><a href=#citation>Citation</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>Semantic segmentation is a crucial component of visual perception, with high demand in both low-latency and precision-focused applications. I have decided to create an overview of the current state of this task.</p><p>The first part contains a brief overview of various approaches to evaluating segmentation model performance. Choosing the right set of metrics is often the first step when developing a segmentation system. Understanding the common performance measurements helps clarify the task at hand and provides insights into the challenges that arise when creating such a system.</p><p>The second part delves into loss functions, a topic closely connected to metrics. Selecting an appropriate loss function greatly impacts a model&rsquo;s ability to improve on a specific metric.</p><p>An overview of the architectures is not included in this post but is likely to be covered in Part 2.</p><h2 id=notation>Notation<a hidden class=anchor aria-hidden=true href=#notation>#</a></h2><table><thead><tr><th style=text-align:left>Syntax</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left>$\Omega = \{x_1,&mldr;,x_N\}$</td><td style=text-align:left>Image represented by a point set</td></tr><tr><td style=text-align:left>$N=|\Omega|$</td><td style=text-align:left>Number of pixels</td></tr><tr><td style=text-align:left>$C$</td><td style=text-align:left>Number of classes</td></tr><tr><td style=text-align:left>$S_g=\left\{y_i| y_i\in\{0,1\} \text{ or }[0,1]\right\}_i^N$</td><td style=text-align:left>Partition of the image representing ground truth segmentation</td></tr><tr><td style=text-align:left>$S_t=\left\{\hat{y}_i| \hat{y}_i\in\{0,1\} \text{ or }[0,1]\right\}_i^N$</td><td style=text-align:left>Partition of the image representing predicted segmentation</td></tr><tr><td style=text-align:left>$S^1=\{y_i| y_i=1\}$</td><td style=text-align:left>Foreground of the segmentation</td></tr><tr><td style=text-align:left>$S^0=\{y_i| y_i=0\}$</td><td style=text-align:left>Background of the segmentation</td></tr><tr><td style=text-align:left>$\mathbf{1}_{\text{condition}}$</td><td style=text-align:left>Binary indication function</td></tr><tr><td style=text-align:left>$A \odot B$</td><td style=text-align:left>Hadamard (element-wise) product</td></tr></tbody></table><h2 id=metrics>Metrics<a hidden class=anchor aria-hidden=true href=#metrics>#</a></h2><p>When evaluating a predicted segmentation against the ground truth, various factors need to be taken into account, depending on the objective. These factors include overall alignment and shape errors, volume errors, boundary errors, density, and the general quality of the segmentation. It is important to note that some metrics are sensitive to outliers, class imbalance, segmentation density, and other specific details.</p><p>This section is based on the survey by [<a href=https://rdcu.be/c9uJP>Taha et al. 2015</a>] and offers definitions for 20 metrics categorized into six groups: overlap-based, volume-based, pair counting-based, information theory-based, probabilistic, and spatial distance-based. While insights on the behavior of each specific metric are provided, readers interested in a more comprehensive overview are encouraged to refer to the original study.</p><h3 id=common-definitions>Common definitions<a hidden class=anchor aria-hidden=true href=#common-definitions>#</a></h3><p>Let the image be represented by a point set $\Omega = \{x_1,&mldr;,x_N\}$ with $|\Omega| = w \times h \times d = N$, where $w$, $h$ and $d$ are the width, height and depth of the grid on which the image is defined.</p><p>Let the ground truth segmentation be represented by the partition $S_g=\left\{y_i|y_i\in\{0,1\} \text{ or }(0,1)\right\}_i^N$ of $\Omega$.
Let the predicted segmentation be represented by the partition $S_t=\left\{\hat{y}_i|\hat{y}_i\in\{0,1\} \text{ or }(0,1)\right\}_i^N$.
Assume that $S^1=\{y_i|y_i=1\}$ is the foreground and $S^0=\{y_i|y_i=0\}$ is the background.</p><h3 id=basic-components-confusion-matrix>Basic components: confusion matrix<a hidden class=anchor aria-hidden=true href=#basic-components-confusion-matrix>#</a></h3><p>For two crisp segmentations $S_g$ and $S_t$, the confusion matrix consists of the four basic components that reflect the overlap between them, namely True Positive, False Positive, False Negative and True Negative. These components are counting based, which is reflected in the case when $\hat{y_i}\in\{0,1\}$. However, I abuse the notation here by pointing out that their soft probabilistic approximations are defined by the same formulas when $\hat{y}_i\in[0,1]$. This distinction will be relevant when discussing loss functions, as the latter are differentiable, whereas the former are not.</p><p>$$
\begin{align}
TP&=\sum_i^N\hat{y}_i y_i\\
FP&=\sum_i^N\hat{y}_i(1-y_i)\\
FN&=\sum_i^N(1-\hat{y}_i) y_i\\
TN&=\sum_i^N(1-\hat{y}_i)(1-y_i) \\
\end{align}
$$</p><p>Note that the fuzzy definitions are available in the original study [<a href=https://rdcu.be/c9uJP>Taha et al. 2015</a>]. I have omitted them for clarity, but it should be assumed that all metrics based on these components are applicable in the fuzzy case as well.</p><p>When evaluating metrics defined using the confusion matrix, it is important to consider that some segmentation algorithms may produce segmentations with lower density. These resulting segmentations may have numerous uniformly distributed holes in the foreground, which are counted as false negatives. Such segmentations will be scored lower than denser counterparts with equivalent volume and alignment. This effect may be undesirable if the objective is to prioritize boundary precision over density. In these cases, spatial distance-based metrics are a more suitable choice.</p><p>Another aspect to consider, specifically for metrics that include TN, is that they reward segmentations with bias towards background and penalize larger segments more heavily. This can be particularly disadvantageous when aiming for maximum recall.</p><h3 id=overlap-based-metrics>Overlap-based metrics<a hidden class=anchor aria-hidden=true href=#overlap-based-metrics>#</a></h3><p>A shared characteristic of all overlap-based metrics is that they do not account for the pixel positions of false positives and false negatives. This is because these pixels are not part of the overlapping region, causing overlap-based metrics to fail in reflecting the distance-wise magnitude of the error. As a result, these metrics are not suitable for situations where low or zero overlap is likely to occur due to alignment errors, such as with class imbalance or when the segmented regions are small (in at least one dimension). As mentioned earlier, they also penalize low-density segmentations.</p><p>However, overlap-based metrics are suitable when dealing with outliers and generally low-quality segmentations, as they reflect poor alignment and low overlap. They are also appropriate when prioritizing volume.</p><h4 id=dice-score-and-intersection-over-union>Dice score and Intersection over Union<a hidden class=anchor aria-hidden=true href=#dice-score-and-intersection-over-union>#</a></h4><p>The Dice coefficient is the most commonly used metrics in segmentation.</p><p>$$
\begin{equation}
Dice = \frac{2\big|S_g^1 \cap S_p^1\big|}{\big| S_g^1 \big| + \big| S_p^1 \big|} = \frac{2TP}{2TP + FP + FN}
\end{equation}
$$</p><p>Similarly, Intersection over Union (IoU), also known as the Jaccard index, is defined as</p><p>$$
\begin{equation}
IoU= \frac{\big|S_g^1 \cap S_p^1\big|}{\big| S_g^1 \cup S_p^1 \big|}=\frac{TP}{TP+FP+FN}
\end{equation}
$$</p><p>The two metrics can be shown to be related:</p><p>$$
\begin{align}
IoU &= \frac{Dice}{2 - Dice}, & Dice &= \frac{2IoU}{1+IoU}
\end{align}
$$</p><p>Therefore there’s no additional information gain in considering both of them at the same time.</p><h4 id=tpr-tnr-fpr-fnr>TPR, TNR, FPR, FNR<a hidden class=anchor aria-hidden=true href=#tpr-tnr-fpr-fnr>#</a></h4><p>True Positive Rate (TPR, Recall, Sensitivity) measures the proportion of positive pixels in the ground truth that are identified as positive in the evaluated segmentation. True Negative Rate (TNR, Specificity) measures the proportion of negative (background) pixels in the ground truth that are identified as negative in the evaluated segmentation. Both of these metrics are highly sensitive to segment size, as they penalize errors in small segments more severely than in larger ones.</p><p>$$
\begin{equation}
TPR = Recall = Sensitivity = \frac{TP}{TP+FN}
\end{equation}
$$</p><p>$$
\begin{equation}
TNR = Specificity = \frac{TN}{TN+FP}
\end{equation}
$$</p><p>In some applications, missing regions are more undesirable than added regions, requiring the segmentation to include at least all true positives and thus prioritizing recall over precision.</p><p>False Positive Rate (FPR) and False Negative Rate are directly related to TPR and TNR, respectively. This implies that one should select at most one metric from each pair for evaluation.</p><p>$$
\begin{equation}
FPR = \frac{FP}{FP+TN} = 1 - TNR
\end{equation}
$$</p><p>$$
\begin{equation}
FNR = \frac{FN}{FN+TP} = 1 - TPR
\end{equation}
$$</p><h4 id=precision>Precision<a hidden class=anchor aria-hidden=true href=#precision>#</a></h4><p>Precision, also know as the Positive Predictive Value (PPV) is used to calculate F-Measure.</p><p>$$
\begin{equation}
Precision = PPV = \frac{TP}{TP+FP}
\end{equation}
$$</p><h4 id=f_beta-measure>$F_{\beta}$-Measure<a hidden class=anchor aria-hidden=true href=#f_beta-measure>#</a></h4><p>$F_{\beta}$-Measure represents the trade-off between precision and recall:</p><p>$$
\begin{equation}
F_{\beta} = \frac{(\beta^2 + 1)\cdot Precision \cdot Recall}{\beta^2 \cdot Precision \cdot Recall}
\end{equation}
$$</p><p>At $\beta=1$ it becomes an $F_1$-Measure, also called the harmonic mean. By substitution it can be shown to be equivalent to the Dice index.</p><p>$$
\begin{equation}
F_1 = \frac{2\cdot Precision \cdot Recall}{Precision \cdot Recall} = Dice
\end{equation}
$$</p><h4 id=global-consistency-error>Global Consistency Error<a hidden class=anchor aria-hidden=true href=#global-consistency-error>#</a></h4><p>Let $R(S,x)$ be a set of all the pixels that belong to the same segment as $x$ in the segmentation $S$ (foreground or background). The error between two segmentations $S_1$ and $S_2$ at the pixel $x$ is defined as follows:</p><p>$$
\begin{equation}
LRE(S_1, S_2, x) = \frac{\left| R(S_1, x) \setminus R(S_2, x) \right|}{\left| R(S_1, x) \right|}
\end{equation}
$$</p><p>Note that it’s not symmetric. Now, the Global Consistency Error (GCE) can be defined as the average $LRE$ over all pixels:</p><p>$$
\begin{equation}
GCE(S_p, S_g) = \frac1N\min \left\{ \sum_i^N LRE(S_p, S_g, x_i), \sum_i^N LRE(S_g, S_p, x_i) \right\}
\end{equation}
$$</p><p>Or, in terms of the confusion matrix,</p><p>$$
\begin{equation}
\begin{split}
GCE(S_p, S_g) = \frac1N\min \bigg\{ &\frac{FN(FN+2TP)}{TP+FN} + \frac{FP(FP+2TN)}{TN+FP}, \\[15pt]
&\frac{FP(FP+2TP)}{TP+FP} + \frac{FN(FN+2TN)}{TN+FN} \bigg\}
\end{split}
\end{equation}
$$</p><h3 id=note-on-the-multiple-label-case>Note on the multiple label case<a hidden class=anchor aria-hidden=true href=#note-on-the-multiple-label-case>#</a></h3><p>In many real-world scenarios, it&rsquo;s common to compare segmentations with multiple labels. In practice, a typical approach is to compare each label individually and then calculate the average across all labels. Assume $M$ is a metric function defined for the binary case,</p><p>$$
\begin{equation}
M_{ml}=\frac1C\sum_cM^c
\end{equation}
$$</p><p>where $C$ is the number of classes.</p><h4 id=generalized-dice-score>Generalized Dice score<a hidden class=anchor aria-hidden=true href=#generalized-dice-score>#</a></h4><p>The approach presented above, however, ignores class imbalance. In literature one can find a weighted multi-label variant of the Dice score called the Generalized Dice score:</p><p>$$
\begin{equation}
GD=\frac{2\sum_cw_c\big|S_g^c \cap S_p^c\big|}{\sum_cw_c\Big[\big| S_g^c \big| + \big| S_t^c \big|\Big]} = \frac{2\sum_cw_cTP_c}{\sum_cw_c[2TP_c + FP_c + FN_c]}
\end{equation}
$$</p><p>where $w_c=\left|S_g^c\right|^{-2} = \left(TP_c+FN_c\right)^{-2}$. Fuzzy definition can be found in the original study. One can obtain a generalized IoU score by applying (7).</p><h3 id=volume-based-metrics>Volume based metrics<a hidden class=anchor aria-hidden=true href=#volume-based-metrics>#</a></h3><h4 id=volumetric-similarity>Volumetric Similarity<a hidden class=anchor aria-hidden=true href=#volumetric-similarity>#</a></h4><p>Volumetric Similarity (VS) is defined as $1-VD$, where $VD$ is the volumetric distance. We calculate it as the difference between the absolute volumes of the segments divided by the sum of these volumes.</p><p>$$
\begin{equation}
VS = 1- \frac{\Big| \left| S_p^1 \right| - \left| S_g^1 \right| \Big|}{\left| S_p^1 \right|+ \left| S_g^1 \right|} = 1 - \frac{\left|FN - FP\right|}{2TP+FP+FN}
\end{equation}
$$</p><p>Note that even though we expressed it through the confusion matrix, VS is not considered to be an overlap metric since it only compares the absolute volumes. In fact, it can reach 1 even at zero overlap.</p><p>It is important to keep in mind that VS only compares the volume of the segments and carries no information about their shape or alignment.</p><h3 id=pixel-pair-groups>Pixel pair groups<a hidden class=anchor aria-hidden=true href=#pixel-pair-groups>#</a></h3><p>In order to advance to the next group of metrics, we are going to define the base pair-counting elements. Let $P$ be the set of $\frac{n(n-1)}{2}$ pairs representing all pixel pairs in $X$. We’re going to classify each pair $(x_i,x_j)\in P\text{, }i,j\in[0,N]$ into one of four categories based on which subset (foreground or background) those pixels are placed to according to each of the segmentations. To avoid the $O(n^2)$ runtime, it is shown to be possible to calculate these categories using the values in the confusion matrix.</p><p>For pairs $(x_i,x_j)$ where $x_i$ and $x_j$ are placed in the same subset in both $S_g$ and $S_t$:</p><p>$$
\begin{equation}
a = \frac{1}{2}\bigg[TP(TP-1)+FP(FP-1)+TN(TN-1)+FN(FN-1)\bigg]
\end{equation}
$$</p><p>Where $x_i$ and $x_j$ are placed in the same subset in $S_g$, but in different subsets in $S_t$:</p><p>$$
\begin{equation}
b = \frac{1}{2}\bigg[ (TP+FN)^2+(TN+FP)^2-\Big(TP^2+TN^2+FP^2+FN^2\Big) \bigg]
\end{equation}
$$</p><p>Where $x_i$ and $x_j$ are placed in different subsets in $S_g$, but in the same subset in $S_t$:</p><p>$$
\begin{equation}
c = \frac{1}{2}\bigg[ (TP+FP)^2+(TN+FN)^2-\Big(TP^2+TN^2+FP^2+FN^2\Big) \bigg]
\end{equation}
$$</p><p>Finally, where $x_i$ and $x_j$ are placed in different subsets in both $S_g$ and $S_t$:</p><p>$$
\begin{equation}
d = \frac{N(N-1)}{2}-(a+b+c)
\end{equation}
$$</p><h3 id=pair-counting-based-metrics>Pair counting based metrics<a hidden class=anchor aria-hidden=true href=#pair-counting-based-metrics>#</a></h3><h4 id=rand-index>Rand Index<a hidden class=anchor aria-hidden=true href=#rand-index>#</a></h4><p>The Rand Index (RI) was originally proposed for measuring the similarity between clusterings and was later adapted for classification.</p><p>$$
\begin{equation}
RI(S_g, S_p) = \frac{a+b}{a+b+c+d}
\end{equation}
$$</p><h4 id=adjusted-rand-index>Adjusted Rand Index<a hidden class=anchor aria-hidden=true href=#adjusted-rand-index>#</a></h4><p>The Adjusted Rand Index (ARI) is a modification of RI with a correction for chance. It can be expressed by the pair-counting groups as:</p><p>$$
\begin{equation}
ARI(S_g, S_p) = \frac{2(ad-bc)}{c^2+b^2+2ad+(a+d)(c+b)}
\end{equation}
$$</p><p>Having the built-in chance adjustment makes ARI a good choice for when there is a heavy class imbalance.</p><h3 id=information-theory-based-metrics>Information theory based metrics<a hidden class=anchor aria-hidden=true href=#information-theory-based-metrics>#</a></h3><p>Assuming $N=TP+FP+FN+TN$ is the total number of pixels, the probability of a randomly sampled pixel belonging to each of the classes in either segmentation can be expressed using the confusion matrix as follows:</p><p>$$
\begin{align}
p\left(S_g^1\right) &= \frac{TP+FN}{N} \\[15pt]
p\left(S_g^0\right) &= \frac{TN+FP}{N} \\[15pt]
p\left(S_t^1\right) &= \frac{TP+FP}{N} \\[15pt]
p\left(S_t^0\right) &= \frac{TN+FN}{N}
\end{align}
$$</p><p>The joint probabilities for some are given by</p><p>$$
\begin{align}
p\left(S_g^1,S_p^1\right) = \frac{TP}{N} \\[15pt]
p\left(S_g^1,S_p^0\right) = \frac{FN}{N} \\[15pt]
p\left(S_g^0,S_p^1\right) = \frac{FP}{N} \\[15pt]
p\left(S_g^0,S_p^0\right) = \frac{TN}{N}
\end{align}
$$</p><p>Having the definitions above we can express the marginal entropy between the regions</p><p>$$
\begin{equation}
H(S)=-\sum_i p\left(S^i\right)\log p\left(S^i\right)
\end{equation}
$$</p><p>and the joint entropy between the segmentations</p><p>$$
\begin{equation}
H(S_1,S_2)=-\sum_{i,j} p\left(S_1^i, S_2^j\right)\log p\left(S_2^i,S_2^j\right)
\end{equation}
$$</p><h4 id=mutual-information>Mutual Information<a hidden class=anchor aria-hidden=true href=#mutual-information>#</a></h4><p>The Mutual Information (MI) measures the reduction in uncertainty of one variable when the other one is known.</p><p>$$
\begin{equation}
MI(S_g,S_p)=H(S_g)+H(S_p)-H(S_g,S_p)
\end{equation}
$$</p><p>Note that MI essentially measures how much information the segmentations have in common and therefore rewards high recall.</p><h4 id=variation-of-information>Variation of Information<a hidden class=anchor aria-hidden=true href=#variation-of-information>#</a></h4><p>The Variation of Information (VoI) measures the amount of information lost when transitioning from one variable to the other.</p><p>$$
\begin{equation}
VoI(S_g,S_p)=H(S_g)+H(S_p)-2MI(S_g,S_p)
\end{equation}
$$</p><h3 id=probabilistic-metrics>Probabilistic metrics<a hidden class=anchor aria-hidden=true href=#probabilistic-metrics>#</a></h3><p><strong>Intraclass Correlation Coefficient</strong></p><p>Intraclass Correlation Coefficient (ICC) is sometimes used as a measure of consistency between two segmentations, specifically in the medical imaging domain.</p><p>$$
\begin{equation}
ICC = \frac{\sigma_S^2}{\sigma_S^2+\sigma_{\epsilon}^2}
\end{equation}
$$</p><p>Here $\sigma_S$ denotes variance caused by differences between segmentations and $\sigma_{\epsilon}$ denotes variance cause by differences between the points within each segmentation. For segmentations $S_g$ and $S_t$, we express it via the mean squares between segmentations $MS_b$ and the mean squares within the segmentations $MS_w$:</p><p>$$
\begin{gather}
&amp;ICC = \frac{MS_b-MS_w}{MS_b+MS_w}
\end{gather}
$$</p><p>where</p><p>$$
\begin{align}
&amp;MS_b = \frac{2}{N-1}\sum_i \left(\frac{\hat{y}_i+y_i}{2}-\mu\right)^2 \\[15pt]
&amp;MS_w = \frac1N\sum_i\left(y_i-\frac{\hat{y}_i+y_i}{2}\right)^2 + \left(\hat{y}_i-\frac{\hat{y}_i+y_i}{2}\right)^2 \\
\end{align}
$$</p><p>Here $\mu$ is the mean of means of the two segmentations.</p><h4 id=probabilistic-distance>Probabilistic Distance<a hidden class=anchor aria-hidden=true href=#probabilistic-distance>#</a></h4><p>The Probabilistic Distance (PBD) [<a href=https://link.springer.com/chapter/10.1007/3-540-45468-3_62>Guido et al. 2001</a>] is designed as a measure of distance between two fuzzy segmentations.</p><p>$$
\begin{equation}
PBD(S_g,S_p)=\frac{\sum_i \big|y_i-\hat{y}_i\big|}{2\sum_i y_i \hat{y}_i}
\end{equation}
$$</p><p>Note that in contrast to Dice, the PBD over-penalizes false positives and false negatives, as they both reduce the denominator and increase the numerator to the point where PBD reaches infinity at zero overlap. This results in PBD strongly reflecting alignment errors, i.e., when the volume is correct and the overlap is low.</p><h4 id=cohens-kappa-coefficient>Cohen’s Kappa Coefficient<a hidden class=anchor aria-hidden=true href=#cohens-kappa-coefficient>#</a></h4><p>The Cohen’s Kappa Coefficient is a robust measure of agreement between the samples that takes into account the agreement caused by chance.</p><p>$$
\begin{equation}
\kappa=\frac{2(TP\cdot TN-FN\cdot FP)}{(TP+FP)(FP+TN)+(TP+FN)(FN+TN)}
\end{equation}
$$</p><p>Chance adjustment makes Kappa a good choice when there is high class imbalance.</p><h4 id=roc-auc>ROC AUC<a hidden class=anchor aria-hidden=true href=#roc-auc>#</a></h4><p>The ROC curve is a plot of TPR against FPR at every possible threshold. The area under the ROC curve reflects the probability for the classifier to rank a positive example higher than the negative one. It is possible to calculate a rough estimate of the AUC for a single measurement case.</p><p>$$
\begin{equation}
AUC=1-\frac{FPR+FNR}{2}
\end{equation}
$$</p><p>Note that calculating AUC this way is generally not recommended, since it significantly underestimates the value.</p><h3 id=spatial-distance-based-metrics>Spatial distance based metrics<a hidden class=anchor aria-hidden=true href=#spatial-distance-based-metrics>#</a></h3><p>Spatial distance-based metrics possess the notable property of taking into account pixel positions outside the overlap region, as well as those within it. This enables them to offer more meaningful rankings in situations where overlap is likely to be low, such as with small object segmentations and low-density segmentations. These metrics are also applicable when prioritizing boundary or overall alignment and in cases involving low-quality segmentations.</p><h4 id=hausdorff-distance>Hausdorff Distance<a hidden class=anchor aria-hidden=true href=#hausdorff-distance>#</a></h4><p>The Hausdorff Distance (HD) is defined as the maximum distance from a point in one set to the nearest point in the other set.</p><p>$$
\begin{equation}
HD(S_g^i,S_p^i)=\max\big(h(S_g^i, S_p^i),h(S_p^i, S_g^i)\big)
\end{equation}
$$</p><p>where $h(A,B)$ is the directed Hausdorff distance given by</p><p>$$
\begin{equation}
h(A,B)=\max_{a\in A}\min_{b\in B} |a-b|
\end{equation}
$$</p><p>HD can be viewed as an indicator of the largest segmentation error. It is computed between boundaries of the ground truth and the predicted segmentation.</p><p><img loading=lazy src=/segmentation-metrics-and-losses/hausdorff.png alt=hausdorff></p><p>Algorithms have been developed that calculate the HD in near-linear time. Note that HD is sensitive to outliers, which is why it is recommended to use the quantile version instead of applying it directly.</p><h4 id=average-hausdorff-distance>Average Hausdorff Distance<a hidden class=anchor aria-hidden=true href=#average-hausdorff-distance>#</a></h4><p>The Average Hausdorff Distance is the HD averaged over all points. It is known to be more robust than the original.</p><p>$$
\begin{equation}
AHD(S_g^i,S_p^i)=\max\big(d(S_g^i,S_p^i),d(S_p^i,S_g^i)\big)
\end{equation}
$$</p><p>where $d(A,B)$ is the directed average Hausdorff distance that is defined as</p><p>$$
\begin{equation}
d(A,B)=\frac{1}{N}\sum_{a\in A}\min_{b\in B}|a-b|
\end{equation}
$$</p><h4 id=mahalanobis-distance>Mahalanobis Distance<a hidden class=anchor aria-hidden=true href=#mahalanobis-distance>#</a></h4><p>The Mahalanobis Distance (MD) is a metric that measures the distance between a point and a distribution while considering the shape of the distribution. However, when comparing image segmentations, we need to measure the distance between two distributions. To achieve this, we calculate the distance between their means as follows:</p><p>$$
\begin{equation}
MD(S_g^i,S_p^i) = \sqrt{(\mu_{g,i}-\mu_{t,i})^T K^{-1}(\mu_{g,i}-\mu_{t,i})}
\end{equation}
$$</p><p>where $\mu_{g,i}$ and $\mu_{t,i}$ are the means for the $i$th category in the respective segmentation, and their common covariance matrix is given by</p><p>$$
\begin{equation}
K=\frac{n_{g,i} K_{g,i} + n_{t,i} K_{t,i}}{n_{g,i} + n_{t,i}}
\end{equation}
$$</p><p>Here $K_{g,i}$ and $K_{t,i}$ are respective covariance matrices and $n_{g,i},n_{t,i}$ are the numbers of pixels.</p><p>Note that MD ignores boundary details and considers only general shape and alignment, i. e. the two ellipsoids that best represent the segmentations.</p><p><img loading=lazy src=/segmentation-metrics-and-losses/summary_table.png alt=summary_table></p><h2 id=loss-functions>Loss functions<a hidden class=anchor aria-hidden=true href=#loss-functions>#</a></h2><p>After examining various popular metrics and gaining insight into the challenges involved, we can proceed to select the appropriate loss function. The choice of loss function can profoundly influence a model&rsquo;s capacity to learn the nuances of a task, which often includes class imbalance, disparate object sizes, and intricate boundary delineation.</p><p>This section draws in part on Jun Ma&rsquo;s survey [<a href=https://arxiv.org/abs/2005.13449>Jun Ma 2020</a>], with some additions. It does not maintain a one-to-one correspondence with the metric section, as numerous metrics discussed earlier are non-differentiable and thus unsuitable for backpropagation. However, some have been approximated with differentiable functions, making them compatible with gradient-based optimization.</p><h3 id=distribution-based-loss>Distribution-based loss<a hidden class=anchor aria-hidden=true href=#distribution-based-loss>#</a></h3><p>Distribution-based loss functions treat both the ground truth and predicted segmentation as probability distributions, with the goal of minimizing their differences. The cross-entropy serves as the foundational loss function in this category, from which others are derived.</p><h4 id=cross-entropy-loss>Cross entropy loss<a hidden class=anchor aria-hidden=true href=#cross-entropy-loss>#</a></h4><p>Cross-entropy (CE) is related to the Kullback-Leibler (KL) divergence, a metric that quantifies the dissimilarity between two probability distributions. In machine learning, when the data distribution is determined by the training set, minimizing KL divergence is equivalent to minimizing CE.</p><p>$$
\begin{equation}
L_{CE}=-\frac1N\sum_i\sum_c y_i^c \log \hat{y}_i^c
\end{equation}
$$</p><p>where $g_i^c$ indicates if class label $c$ is a correct classification for the pixel $i$, and $s_i^c$ is the corresponding predicted probability.</p><p>Weighted cross entropy (WCE) [<a href=https://arxiv.org/abs/1505.04597>Ronneberger et al. 2015</a>] is a popular modification to the CE:</p><p>$$
\begin{equation}
L_{WCE}=-\frac1N\sum_i\sum_c w_c y_i^c \log \hat{y}_i^c
\end{equation}
$$</p><p>where $w_c$ is the corresponding class weight. It is common to set $w_c$ as inversely proportional to the class frequency to balance out majority classes.</p><h4 id=topk-loss>TopK loss<a hidden class=anchor aria-hidden=true href=#topk-loss>#</a></h4><p>TopK loss [<a href=https://arxiv.org/abs/1605.06885>Wu et al. 2016</a>] focuses the training on the hard pixels by discarding the ones that model evaluates with enough confidence. Notably this approach automatically balances the biased data by skipping the over-learned majority class.</p><p>$$
\begin{equation}
L_{TopK}= -\frac{\sum_i\sum_c \bm{1}_{\hat{y}_i^c&lt;t} \log \hat{y}_i^c}{\sum_i\sum_c \bm{1}_{\hat{y}_i^c&lt;t}}
\end{equation}
$$</p><p>where $t\in(0,1]$ is the discarding threshold and $\bm{1}_{\dots}$ is the binary indication function.</p><h4 id=focal-loss>Focal loss<a hidden class=anchor aria-hidden=true href=#focal-loss>#</a></h4><p>The Focal loss [<a href=https://arxiv.org/abs/1708.02002v2>Lin et al. 2017</a>] directly addresses class imbalance by adding an extra multiplier to the CE. This multiplier diminishes the loss value for the well classified examples, effectively the function heavily towards the hard samples.</p><p>$$
\begin{equation}
L_{focal} = -\frac1N \sum_i\sum_c \big(1-\hat{y}_i^c\big)^{\gamma} y_i^c\log \hat{y}_i^c
\end{equation}
$$</p><h3 id=overlap-based-loss>Overlap-based loss<a hidden class=anchor aria-hidden=true href=#overlap-based-loss>#</a></h3><p>Overlap-based loss functions, such as the popular Dice loss, aim to directly optimize overlap metrics. However, these metrics are count-based, making them non-differentiable and unsuitable for gradient-based optimization.</p><p>A common solution to this issue, with a few exceptions, involves using a surrogate function to create a smooth approximation of the confusion matrix elements. Although these soft versions don&rsquo;t precisely match the original representations, they closely resemble them, making optimization feasible.</p><p>Note that in this section equations (1) - (4) should be interpreted as defined on soft probabilistic labels $\hat{y}_i\in[0,1]$. This interpretation makes these equations differentiable with respect to both the predictions and the model parameters, enabling their use in gradient-based optimization during model training.</p><h4 id=sensitivity-specificity-loss>Sensitivity-specificity loss<a hidden class=anchor aria-hidden=true href=#sensitivity-specificity-loss>#</a></h4><p>Sensitivity-specificity (SS) loss [<a href=https://link.springer.com/chapter/10.1007/978-3-319-24574-4_1>Brosch et al. 2015</a>] combines both of these metrics to facilitate training with heavy class imbalance.</p><p>$$
\begin{equation}
L_{SS}=r\cdot \frac{\sum_i(y_i-\hat{y}_i)^2y_i}{\sum_i y_i}+(1-r)\cdot \frac{\sum_i(y_i-\hat{y}_i)^2(1-y_i)}{\sum_i (1-y_i)}
\end{equation}
$$</p><p>where $r$ denotes the weight and is set to 0.05 by default.</p><h4 id=dice-loss>Dice loss<a hidden class=anchor aria-hidden=true href=#dice-loss>#</a></h4><p>The Dice loss [<a href=https://arxiv.org/abs/1606.04797>Milletari et al. 2016</a>] optimizes the Dice coefficient by utilizing a soft approximation. It accounts for class imbalance by definition.</p><p>$$
\begin{equation}
L_{Dice}=1-Dice
\end{equation}
$$</p><h4 id=iou-loss>IoU loss<a hidden class=anchor aria-hidden=true href=#iou-loss>#</a></h4><p>IoU loss [<a href=https://link.springer.com/chapter/10.1007/978-3-319-50835-1_22>Rahman et al. 2016</a>] is defined similarly to the Dice loss.</p><p>$$
\begin{equation}
L_{IoU}=1-IoU
\end{equation}
$$</p><h4 id=tversky-loss>Tversky loss<a hidden class=anchor aria-hidden=true href=#tversky-loss>#</a></h4><p>Tversky loss [<a href=https://arxiv.org/abs/1706.05721>Salehi et al. 2017</a>] adds additional parameters to control the trade-off between false positives and false negatives, since assigning the same weight to false positives and false negatives may result in low recall for small regions. When $\alpha=\beta=0.5$ the Tversky loss becomes equivalent to the Dice loss, and when $\alpha=\beta=1$, it becomes equivalent to the Jaccard loss.</p><p>$$
\begin{equation}
Tversky(S_g,S_p;\alpha,\beta)=\frac{TP}{TP+\alpha FP + \beta FN}
\end{equation}
$$</p><p>$$
\begin{equation}
L_{Tversky}(S_g,S_p;\alpha,\beta)=1-Tversky
\end{equation}
$$</p><h4 id=generalized-dice-loss>Generalized Dice loss<a hidden class=anchor aria-hidden=true href=#generalized-dice-loss>#</a></h4><p>Generalized Dice loss optimized the multiclass extension of the Dice coefficient.</p><p>$$
\begin{equation}
L_{GD}=1-GD
\end{equation}
$$</p><h4 id=focal-tversky-loss>Focal Tversky loss<a hidden class=anchor aria-hidden=true href=#focal-tversky-loss>#</a></h4><p>The Focal Tversky loss [<a href=https://arxiv.org/abs/1810.07842>Abraham et al. 2018</a>] is an extension of the Tversky loss, specifically tailored to enhance performance when dealing with smaller objects. It uses the focusing parameted $\gamma\in[1,3]$ to put heavier emphasis on the hard misclassifier examples.</p><p>$$
\begin{equation}
L_{FTL}=\big(1-Tversky\big)^{\frac1\gamma}
\end{equation}
$$</p><h4 id=asymmetric-similarity-loss>Asymmetric similarity loss<a hidden class=anchor aria-hidden=true href=#asymmetric-similarity-loss>#</a></h4><p>Asymmetric similarity loss [<a href=https://arxiv.org/abs/1803.11078>Hashemi et al. 2018</a>] is a modified version of Tversky loss that establishes more precise rules for weighting false positives (FP) and false negatives (FN), placing greater emphasis on reducing the latter. Suitable values for the hyperparameter beta can be determined based on class imbalance ratios, ensuring a more balanced approach to addressing discrepancies between classes.</p><p>$$
\begin{equation}
L_{Asym}(S_g,S_p;\beta)=1-\frac{TP}{TP+ \frac{\beta^2}{1+\beta^2}FP + \frac{1}{1+\beta^2}FN}
\end{equation}
$$</p><h4 id=penalty-loss>Penalty loss<a hidden class=anchor aria-hidden=true href=#penalty-loss>#</a></h4><p>The Penalty loss [<a href="https://openreview.net/forum?id=H1lTh8unKN">Yang et al. 2019</a>] builds on the Generalized Dice loss by incorporating a coefficient k, which facilitates adding extra weight for false positives (FP) and false negatives (FN).</p><p>$$
\begin{equation}
L_{pGD}=\frac{L_{GD}}{1+k\left(1-L_{GD}\right)}
\end{equation}
$$</p><h4 id=lovász-hinge-loss>Lovász Hinge loss<a hidden class=anchor aria-hidden=true href=#lovász-hinge-loss>#</a></h4><p>Lovász Hinge loss [<a href=https://arxiv.org/abs/1705.08790>Berman et al. 2017</a>] offers an alternative differentiable surrogate function for IoU, distinct from the IoU loss itself. In this approach, the IoU loss is reformulated as a set function on the set of mispredictions. It can be shown that such a set function is submodular, therefore its tight convex hull can be computed efficiently as its Lovász extension.</p><p>Denote $\Delta: \{0,1\}^N\rarr\mathbb{R}$ the rewritten IoU loss. We calculate its Lovász extension as</p><p>$$
\begin{equation}
\overline{\Delta} = \sum_i^N m_ig_i(\bm{m})
\end{equation}
$$</p><p>Here vector $\bm{g}(\bm{m})$ is the derivative of $\overline{\Delta}$ with respect to $\bm{m}$.</p><p>$$
\begin{equation}
g_i(\bm{m})=\Delta(\{\pi_1,\dots,\pi_i\})-\Delta(\{\pi_1,\dots,\pi_{i-1}\})
\end{equation}
$$</p><p>$\bm{\pi}$ is a permutation ordering of the components of $\bm{m}$ in decreasing order. $\bm{m}$ is a vector of all pixel errors, that is calculated as follows:</p><ul><li>Rewrite ground truth labels as $y_i^*\in\{-1, 1\}$,</li><li>Denote $F_i$ the logit, such that the predicted label $\hat{y}_i^*=\textrm{sign}(F_i)$,</li><li>$m_i=\max \big(1-F_i\cdot y_i^*,0\big)$</li></ul><p>The obtained function is the the Lovász hinge applied to the IoU loss.</p><p>$$
\begin{equation}
L_{Lovász}=\overline{\Delta}(\bm{m}(\bm{F}))
\end{equation}
$$</p><p>$\overline{\Delta}(\bm{m})$ is a sum of errors $\bm{m}$ that weights them according to the interpolated discrete loss.</p><p>The algorithm above gets a bit more clear when put in code, provided below. Note that some technical details were omitted for clarity.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>lovasz_grad</span><span class=p>(</span><span class=n>gt_sorted</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Compute gradient of the Lovasz extension w.r.t sorted errors
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>gt_sorted</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gts</span> <span class=o>=</span> <span class=n>gt_sorted</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>intersection</span> <span class=o>=</span> <span class=n>gts</span> <span class=o>-</span> <span class=n>gt_sorted</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>union</span> <span class=o>=</span> <span class=n>gts</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>gt_sorted</span><span class=p>)</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>iou_loss</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>-</span> <span class=n>intersection</span> <span class=o>/</span> <span class=n>union</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>iou_loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>lovasz_hinge</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Binary Lovasz hinge loss
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        logits: logits at each prediction (-infinity, +infinity)
</span></span></span><span class=line><span class=cl><span class=s2>        labels: binary ground truth labels {0, 1}
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>signs</span> <span class=o>=</span> <span class=mf>2.0</span> <span class=o>*</span> <span class=n>labels</span> <span class=o>-</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>    <span class=n>errors</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>-</span> <span class=n>logits</span> <span class=o>*</span> <span class=n>signs</span>
</span></span><span class=line><span class=cl>    <span class=n>errors_sorted</span><span class=p>,</span> <span class=n>perm</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>errors</span><span class=p>,</span> <span class=n>descending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>gt_sorted</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[</span><span class=n>perm</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>grad</span> <span class=o>=</span> <span class=n>lovasz_grad</span><span class=p>(</span><span class=n>gt_sorted</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>errors_sorted</span><span class=p>),</span> <span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></div><h4 id=exponential-logarithmic-loss>Exponential Logarithmic Loss<a hidden class=anchor aria-hidden=true href=#exponential-logarithmic-loss>#</a></h4><p>Exponential Logarithmic Loss [<a href=https://arxiv.org/abs/1809.00076>Wong et al. 2018</a>] is a combination of the exponential logarithmic Dice loss and the weighted exponential cross-entropy loss. It has been proposed to enhance segmentation accuracy for small structures in tasks with significant variability in the sizes of objects to be segmented, ensuring a more balanced performance across different scales.</p><p>$$
\begin{equation}
L_{Exp}=w_{eld}L_{eld}+w_{elce}L_{elce}
\end{equation}
$$</p><p>$$
\begin{equation}
L_{eld}=\frac1N \big(-\ln( Dice)^{\gamma_{eld}}\big)
\end{equation}
$$</p><p>$$
\begin{equation}
L_{elce}=\frac1{2N} \Big[w_1 (-\ln\hat{y})^{\gamma_{elce}}+w_2\big(-\ln(1-\hat{y})\big)^{\gamma_{elce}}\Big]
\end{equation}
$$</p><p>where $w_c=\left(\frac{\sum_k f_k}{f_c}\right)^{\frac12}$ is the weight to increase the influence of the rare labels, $f_k$ is the frequency of the label $k$.</p><h4 id=matthews-correlation-coefficient>Matthews correlation coefficient<a hidden class=anchor aria-hidden=true href=#matthews-correlation-coefficient>#</a></h4><p>Matthews correlation coefficient loss [<a href=https://arxiv.org/abs/2010.13454>Abhishek et al. 2020</a>] addresses the fact that the Dice
loss does not include a penalty for misclassifying the false negative pixels, which affects the accuracy of background segmentation.</p><p>$$
\begin{equation}
MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\end{equation}
$$</p><p>$$
\begin{equation}
L_{MCC}=1-MCC
\end{equation}
$$</p><h3 id=compound-loss>Compound loss<a hidden class=anchor aria-hidden=true href=#compound-loss>#</a></h3><p>Achieving a good balance of qualities in a loss function can often be accomplished by combining several of them with assigned weights. Various combinations have proven successful, and below is one such example.</p><h4 id=combo-loss>Combo loss<a hidden class=anchor aria-hidden=true href=#combo-loss>#</a></h4><p>The Combo loss [<a href=https://arxiv.org/abs/1805.02798>Taghanaki et al. 2018</a>] aims to tackle class imbalance and offers control over the tradeoff between false positives and false negatives.</p><p>$$
\begin{equation}
\begin{split}
L_{Combo}=-\alpha\left(\frac1N\sum_i^N\beta(y_i \log \hat{y}_i)+(1-\beta)(1-y_i)\log(1-\hat{y}_i)\right)- \\[15pt]
-(1-\alpha)Dice
\end{split}
\end{equation}
$$</p><h3 id=boundary-based-loss>Boundary-based loss<a hidden class=anchor aria-hidden=true href=#boundary-based-loss>#</a></h3><p>Losses in this category primarily rely on the distance transform. Essentially, the distance transform converts a binary segmentation map into a distance map, where each pixel is assigned a value representing its distance to the nearest foreground pixel. However, each approach utilizes the distance transform differently, combining and inverting maps and integrating them into the objective function in unique ways.</p><h4 id=distance-map-penalized-cross-entropy-loss-dpce>Distance map penalized cross entropy loss (DPCE)<a hidden class=anchor aria-hidden=true href=#distance-map-penalized-cross-entropy-loss-dpce>#</a></h4><p>Distance maps for DPCE [<a href=https://arxiv.org/abs/1908.03679>Caliva et al. 2019</a>] are generated by computing the distance transform on the segmentation masks and then inverting them by pixel-wise subtracting the binary segmentation from the mask&rsquo;s maximum distance value. This process aims to create a distance mask where pixels close to the foreground are assigned higher weight compared to those further away. A similar procedure is conducted on the inverted version of the segmentation mask to calculate a distance map inside the foreground regions. The resulting maps are then applied to the loss with element-wise multiplication, imposing heavier penalties for errors near the boundary.</p><p>$$
\begin{equation}
L_{DPCE}=-\frac1N\sum_{i=1}^N(1+\Phi)\odot\sum_c^C y_i^c\log \hat{y}_i^c
\end{equation}
$$</p><p>where $\Phi$ is the distance penalty and $\odot$ denotes the element-wise product.</p><p><img loading=lazy src=/segmentation-metrics-and-losses/dpce.png alt=dpce.png></p><h4 id=boundary-loss>Boundary loss<a hidden class=anchor aria-hidden=true href=#boundary-loss>#</a></h4><p>Distance maps for the Boundary loss [<a href=https://arxiv.org/abs/1812.07032>Kervadec et al. 2018</a>] are constructed as follows: $\phi_G=-D_G(q)$ if $q\in S_g^1$ and $\phi_G=D_G(q)$ otherwise. Here, the distance transform on the ground truth is combined with a negative signed distance transform on the inverted map.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>dist</span><span class=p>(</span><span class=n>seg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>seg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>res</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>seg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>C</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>posmask</span> <span class=o>=</span> <span class=n>seg</span><span class=p>[</span><span class=n>c</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>bool</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>posmask</span><span class=o>.</span><span class=n>any</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>negmask</span> <span class=o>=</span> <span class=o>~</span><span class=n>posmask</span>
</span></span><span class=line><span class=cl>            <span class=n>res</span><span class=p>[</span><span class=n>c</span><span class=p>]</span> <span class=o>=</span> <span class=n>distance</span><span class=p>(</span><span class=n>negmask</span><span class=p>)</span> <span class=o>*</span> <span class=n>negmask</span> <span class=o>-</span> <span class=p>(</span><span class=n>distance</span><span class=p>(</span><span class=n>posmask</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>posmask</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>res</span>
</span></span></code></pre></div><p>The main advantage of the Boundary loss is its ability to bypass the class imbalance issue by focusing on the boundaries rather than the regions themselves.</p><p>$$
\begin{equation}
L_{DB}=\frac1N \sum\phi_G\odot \hat{S}
\end{equation}
$$</p><h4 id=hausdorff-distance-hd-loss>Hausdorff distance (HD) loss<a hidden class=anchor aria-hidden=true href=#hausdorff-distance-hd-loss>#</a></h4><p>Minimizing HD directly can be intractable and may result in unstable training. Nonetheless, it can be approximated using the distance transforms of the ground truth and predicted segmentation [<a href=https://arxiv.org/abs/1904.10030>Karimi et al. 2019</a>]. $d_p$ and $d_g$ denote regular distance transforms on their respective segmentation maps. Parameter $\alpha$ determines the emphasis placed on larger errors and is set to 2 by default. Since recomputing $d_p$ at every step can be expensive, the authors also propose a one-sided variant of this loss.</p><p>$$
\begin{equation}
L_{HD}=\frac1N\sum_{i=1}^N\Big(\left(\hat{y}_i-y_i\right)^2\odot\left(d_p^{\alpha}+d_g^{\alpha}\right)\Big)
\end{equation}
$$</p><h3 id=topological-loss>Topological loss<a hidden class=anchor aria-hidden=true href=#topological-loss>#</a></h3><p>This class of loss functions focuses on ensuring the topological correctness of the predicted segmentation. Specifically, it compares two segmentations to confirm that they have the same number of connected components and holes, which is also referred to as the Betti number.</p><h4 id=topology-preserving-loss>Topology-Preserving loss<a hidden class=anchor aria-hidden=true href=#topology-preserving-loss>#</a></h4><p>Since the Betti number is discrete, it cannot be directly used for gradient-based optimization. To overcome this issue, one can employ persistent homology [<a href=https://arxiv.org/abs/1906.05404>Hu et al. 2019</a>]. In this approach, instead of using a single threshold to obtain the predicted segmentation map, all thresholds are considered. As the threshold decreases, certain topological components &ldquo;get born&rdquo; (a separate disconnected component appears, a cycle gets bridged) and &ldquo;die&rdquo; (one component merges into another, a hole gets filled). We record the threshold value at which such an event occurs as birth time and death time of a component. A full set of these values for every threshold forms a persistence diagram. Each component can be visualized as a dot by plotting the birth time against the death time. It is important to note that, for the ground truth, all components land on the same spot, since they remain unchanged for every value of the threshold.</p><p><img loading=lazy src=/segmentation-metrics-and-losses/topo.png alt=topo.png></p><p>Next, the optimal one-to-one correspondence between the ground truth and the predicted segmentation is determined, with any residual predicted points being matched to the diagonal line. The loss is then calculated as the combined Euclidean distance between each pair of corresponding points.</p><p>$$
\begin{equation}
L_{Topo}=\sum_{p\in Dgm(f)} \Big[birth(p)-birth\big(\gamma^{\star}(p)\big)\Big]^2 + \Big[\big(\gamma^{\star}(p)\big)\Big]^2
\end{equation}
$$</p><p>where $\gamma^{\star}$ is the optimal matching between two different point sets.</p><p><img loading=lazy src=/segmentation-metrics-and-losses/topo2.png alt=topo2.png></p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>Cited as:</p><blockquote><p>Buzin, Andrey. (Apr 2023). Semantic Segmentation: Metrics and Losses in 40 Easy Steps. Computer Visions. <a href=https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/>https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/</a>.</p></blockquote><p>Or</p><pre tabindex=0><code>@article{buzin2023segmentationmetrics,
    title   = &#34;Semantic Segmentation: Metrics and Losses in 40 Easy Steps&#34;,
    author  = &#34;Buzin, Andrey&#34;,
    journal = &#34;deepbuzin.github.io&#34;,
    year    = &#34;2023&#34;,
    month   = &#34;Apr&#34;,
    url     = &#34;https://deepbuzin.github.io/posts/segmentation-metrics-and-losses/&#34;
}
</code></pre><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Taha, A.A., Hanbury, A. <a href=https://rdcu.be/c9uJP>“Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool”</a> BMC Med Imaging **(2015).</p><p>[2] Guido Gerig, et al. <a href=https://link.springer.com/chapter/10.1007/3-540-45468-3_62>“Valmet: A New Validation Tool for Assessing and Improving 3D Object Segmentation”</a> MICCAI 2001.</p><p>[3] Jun Ma <a href=https://arxiv.org/abs/2005.13449>“Segmentation Loss Odyssey”</a> arXiv:2005.13449 (2020).</p><p>[4] Olaf Ronneberger, et al. <a href=https://arxiv.org/abs/1505.04597>“U-Net: Convolutional Networks for Biomedical Image Segmentation”</a> arXiv:1505.04597 (2015).</p><p>[5] Zifeng Wu, et al. <a href=https://arxiv.org/abs/1605.06885>“Bridging Category-level and Instance-level Semantic Image Segmentation”</a> arXiv:1605.06885 (2016).</p><p>[6] Tsung-Yi Lin, et al. <a href=https://arxiv.org/abs/1708.02002v2>“Focal Loss for Dense Object Detection”</a> arXiv:1708.02002 (2017).</p><p>[7] Tom Brosch, et al. <a href=https://link.springer.com/chapter/10.1007/978-3-319-24574-4_1>“Deep Convolutional Encoder Networks for Multiple Sclerosis Lesion Segmentation”</a> MICCAI 2015.</p><p>[8] Fausto Milletari, et al. <a href=https://arxiv.org/abs/1606.04797>“V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation”</a> arXiv:1606.04797 (2016).</p><p>[9] Rahman, M.A., Wang, Y. <a href=https://link.springer.com/chapter/10.1007/978-3-319-50835-1_22>“Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation.”</a> ISVC 2016.</p><p>[10] Seyed Sadegh Mohseni Salehi, et al. <a href=https://arxiv.org/abs/1706.05721>“Tversky loss function for image segmentation using 3D fully convolutional deep networks”</a> arXiv:1706.05721 (2017).</p><p>[11] Nabila Abraham, et al. <a href=https://arxiv.org/abs/1810.07842>“A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation”</a> arXiv:1810.07842 (2018).</p><p>[12] Seyed Raein Hashemi, et al. <a href=https://arxiv.org/abs/1803.11078>“Asymmetric Loss Functions and Deep Densely Connected Networks for Highly Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis Lesion Detection”</a> arXiv:1803.11078 (2018).</p><p>[13] Su Yang, et al. <a href="https://openreview.net/forum?id=H1lTh8unKN">“Major Vessel Segmentation on X-ray Coronary Angiography using Deep Networks with a Novel Penalty Loss Function”</a> MIDL 2019.</p><p>[14] Maxim Berman, et al. <a href=https://arxiv.org/abs/1705.08790>“The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks”</a> arXiv:1705.08790 (2017).</p><p>[15] Ken C. L. Wong, et al. <a href=https://arxiv.org/abs/1809.00076>“3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes”</a> arXiv:1809.00076 (2018).</p><p>[16] Kumar Abhishek, et al. <a href=https://arxiv.org/abs/2010.13454>“Matthews Correlation Coefficient Loss for Deep Convolutional Networks: Application to Skin Lesion Segmentation”</a> arXiv:2010.13454 (2020).</p><p>[17] Saeid Asgari Taghanaki, et al. <a href=https://arxiv.org/abs/1805.02798>“Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation”</a> arXiv:1805.02798 (2018).</p><p>[18] Francesco Caliva, et al. <a href=https://arxiv.org/abs/1908.03679>“Distance Map Loss Penalty Term for Semantic Segmentation”</a> arXiv:1908.03679 (2019).</p><p>[19] Hoel Kervadec, et al. <a href=https://arxiv.org/abs/1812.07032>“Boundary loss for highly unbalanced segmentation”</a> arXiv:1812.07032 (2018)</p><p>[20] Davood Karimi, et al. <a href=https://arxiv.org/abs/1904.10030>“Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks”</a> arXiv:1904.10030 (2019).</p><p>[21] Xiaoling Hu, et al. <a href=https://arxiv.org/abs/1906.05404>“Topology-Preserving Deep Image Segmentation”</a> arXiv:1906.05404 (2019).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://deepbuzin.github.io/tags/segmentation/>segmentation</a></li><li><a href=https://deepbuzin.github.io/tags/metrics/>metrics</a></li><li><a href=https://deepbuzin.github.io/tags/losses/>losses</a></li><li><a href=https://deepbuzin.github.io/tags/deep-learning/>deep learning</a></li><li><a href=https://deepbuzin.github.io/tags/computer-vision/>computer vision</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Segmentation: Metrics and Losses in 40 Easy Steps on twitter" href="https://twitter.com/intent/tweet/?text=Segmentation%3a%20Metrics%20and%20Losses%20in%2040%20Easy%20Steps&amp;url=https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f&amp;hashtags=segmentation%2cmetrics%2closses%2cdeeplearning%2ccomputervision"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Segmentation: Metrics and Losses in 40 Easy Steps on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f&amp;title=Segmentation%3a%20Metrics%20and%20Losses%20in%2040%20Easy%20Steps&amp;summary=Segmentation%3a%20Metrics%20and%20Losses%20in%2040%20Easy%20Steps&amp;source=https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Segmentation: Metrics and Losses in 40 Easy Steps on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f&title=Segmentation%3a%20Metrics%20and%20Losses%20in%2040%20Easy%20Steps"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Segmentation: Metrics and Losses in 40 Easy Steps on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Segmentation: Metrics and Losses in 40 Easy Steps on whatsapp" href="https://api.whatsapp.com/send?text=Segmentation%3a%20Metrics%20and%20Losses%20in%2040%20Easy%20Steps%20-%20https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Segmentation: Metrics and Losses in 40 Easy Steps on telegram" href="https://telegram.me/share/url?text=Segmentation%3a%20Metrics%20and%20Losses%20in%2040%20Easy%20Steps&amp;url=https%3a%2f%2fdeepbuzin.github.io%2fposts%2fsegmentation-metrics-and-losses%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://deepbuzin.github.io/>Computer Visions</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>